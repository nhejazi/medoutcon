utils::globalVariables(c("..w_names", "A", "Z"))

#' TMLE for stochastic (in)direct effects under intermediate confounding
#'
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, M, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated as part of a call to the user-facing
#'  wrapper function \code{\link{medoutcon}}.
#' @param contrast A \code{numeric} double indicating the two values of the
#'  intervention \code{A} to be compared. The default value of \code{NULL} has
#'  no effect, as the value of the argument \code{effect} is instead used to
#'  define the contrasts. To override \code{effect}, provide a \code{numeric}
#'  double vector, giving the values of a' and a* (e.g., \code{c(0, 1)}.
#' @param g_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, for use in fitting a model for the propensity
#'  score, i.e., \eqn{g = P(A | W)}.
#' @param e_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a cleverly parameterized
#'  propensity score that includes the mediators, i.e., \eqn{e = P(A | Z, W)}.
#' @param m_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting the outcome regression.
#' @param q_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, for use in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{q(L | A', W)}.
#' @param r_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a regression involving
#'  the mediator-outcome confounder, i.e., \eqn{r(L | A', M, W)}.
#' @param u_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a reduced regression
#'  useful for computing the efficient one-step estimator, i.e.,
#'  \eqn{u(L, A, W) = E[m(A, L, Z, W) * (q(L|A,W) / r(L|A,Z,W)) *
#'  (e(a'|Z,W) / e(A|Z,W)) | L = l, A = a, W = w]}.
#' @param v_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a reduced regression
#'  useful for computing the efficient one-step estimator, i.e.,
#'  \eqn{v(A,W) = E[\int_z m(a', l, Z, W) * q(l|A',W) d\nu(z) | A = a, W = w)]}.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function
#'  \code{\link{medoutcon}}.
#' @param m_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (M). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{\link{medoutcon}}.
#' @param ext_weights A \code{numeric} vector of observation-level weights that
#'  have been computed externally. Such weights are used in the construction of
#'  a re-weighted one-step estimator or in solving a re-weighted estimating
#'  equation in the case of the TML estimator. Use with caution.
#' @param cv_folds A \code{numeric} integer value specifying the number of folds
#'  to be created for cross-validation. Use of cross-validation / cross-fitting
#'  allows for entropy conditions on the one-step estimator to be relaxed. Note:
#'  for compatibility with \code{origami::make_folds}, this value specified here
#'  must be greater than or equal to 2; the default is to create 10 folds.
#' @param max_iter A \code{numeric} integer giving the maximum number of steps
#'  to be taken for the iterative procedure to construct a TML estimator.
#'
#' @importFrom dplyr "%>%"
#' @importFrom stats var glm as.formula qlogis plogis coef weighted.mean
#' @importFrom origami make_folds cross_validate folds_vfold
#'
est_tml <- function(data,
                    contrast,
                    g_learners,
                    e_learners,
                    m_learners,
                    q_learners,
                    r_learners,
                    u_learners,
                    v_learners,
                    w_names,
                    m_names,
                    ext_weights = NULL,
                    cv_folds = 10,
                    max_iter = 5) {
  # create folds for use with origami::cross_validate
  folds <- origami::make_folds(data,
    fold_fun = origami::folds_vfold,
    V = cv_folds
  )

  # perform the cv_eif procedure on a per-fold basis
  cv_eif_results <- origami::cross_validate(
    cv_fun = cv_eif_tml,
    folds = folds,
    data_in = data,
    contrast = contrast,
    g_learners = g_learners,
    e_learners = e_learners,
    m_learners = m_learners,
    q_learners = q_learners,
    r_learners = r_learners,
    u_learners = u_learners,
    v_learners = v_learners,
    w_names = w_names,
    m_names = m_names,
    use_future = FALSE,
    .combine = FALSE
  )

  # concatenate nuisance function and influence function estimates across folds
  cv_est <- do.call(rbind, cv_eif_results[[1]])

  # generate inverse weights
  ipw_prime <- as.numeric(data$A == contrast[1]) / cv_est$g_prime
  ipw_star <- as.numeric(data$A == contrast[2]) / cv_est$g_star

  # extract nuisance function estimates
  q_prime_Z_one <- cv_est$q_prime_Z_one
  q_prime <- cv_est$q_prime_Z_natural
  u_prime_diff <- cv_est$u_diff
  m_prime <- cv_est$m_prime %>%
    scale_to_unit()
  m_prime_Z_one <- cv_est$m_prime_Z_one %>%
    scale_to_unit()
  m_prime_Z_zero <- cv_est$m_prime_Z_zero %>%
    scale_to_unit()
  y_scaled <- data$Y %>%
    scale_to_unit() %>%
    bound_precision()
  v_star_logit <- cv_est$v_star %>%
    scale_to_unit() %>%
    bound_precision() %>%
    stats::qlogis()
  h_star_mult <- with(cv_est, (g_prime / g_star) * (e_star / e_prime))

  # prepare for iterative targeting
  n_iter <- 0
  eif_stop_crit <- FALSE

  # perform iterative targeting for TMLE
  while (!eif_stop_crit && n_iter <= max_iter) {
    # extract necessary nuisance function components and build h*
    h_star <- (q_prime / cv_est$r_prime_Z_natural) * h_star_mult
    h_star_Z_one <- (q_prime_Z_one / cv_est$r_prime_Z_one) * h_star_mult
    h_star_Z_zero <- (1 - q_prime_Z_one / (1 - cv_est$r_prime_Z_one)) *
      h_star_mult
    m_prime_logit <- m_prime %>%
      bound_precision() %>%
      stats::qlogis()
    m_prime_Z_one_logit <- m_prime_Z_one %>%
      bound_precision() %>%
      stats::qlogis()
    m_prime_Z_zero_logit <- m_prime_Z_zero %>%
      bound_precision() %>%
      stats::qlogis()

    # first fluctuation/tilting step
    # fit first fluctuation/tilting model
    suppressWarnings(
      m_tilt_fit <- stats::glm(
        stats::as.formula("y_scaled ~ -1 + offset(m_prime_logit) + h_star"),
        data = data.table::as.data.table(list(
          y_scaled = y_scaled,
          m_prime_logit = m_prime_logit,
          h_star = h_star
        )),
        subset = data$A == contrast[1],
        weights = data$obs_weights / cv_est$g_prime,
        family = "binomial",
        start = 0
      )
    )

    # second fluctuation/tilting step
    q_prime_logit <- q_prime_Z_one %>%
      scale_to_unit() %>%
      bound_precision() %>%
      stats::qlogis()
    # fit second fluctuation/tilting model
    suppressWarnings(
      q_tilt_fit <- stats::glm(
        stats::as.formula("Z ~ -1 + offset(q_prime_logit) + u_prime_diff"),
        data = data.table::as.data.table(list(
          Z = data$Z,
          u_prime_diff = u_prime_diff,
          q_prime_logit = q_prime_logit
        )),
        subset = data$A == contrast[1],
        weights = data$obs_weights / cv_est$g_prime,
        family = "binomial",
        start = 0
      )
    )

    # extract coefficients
    q_coef <- unname(stats::coef(q_tilt_fit))
    m_coef <- unname(stats::coef(m_tilt_fit))
    if (is.na(q_coef)) q_coef <- 0
    if (is.na(m_coef)) m_coef <- 0

    # re-scale and update nuisance function estimates based on tilting models
    q_prime_Z_one <- stats::plogis(q_prime_logit + q_coef * u_prime_diff)
    q_prime <- data$Z * q_prime_Z_one + (1 - data$Z) * (1 - q_prime_Z_one)
    m_prime <- stats::plogis(m_prime_logit + m_coef * h_star)
    m_prime_Z_one <- stats::plogis(m_prime_Z_one_logit + m_coef * h_star_Z_one)
    m_prime_Z_zero <- stats::plogis(m_prime_Z_zero_logit + m_coef *
      h_star_Z_zero)

    # iterate the iterator
    n_iter <- n_iter + 1

    # NOTE: interesting / non-standard stopping criterion
    eif_stop_crit <- max(abs(c(m_coef, q_coef))) < 0.001 / nrow(data)^(0.6)
  }

  # compute updated substitution estimator and prepare for tilting regression
  v_pseudo <- (m_prime_Z_one * q_prime_Z_one) + m_prime_Z_zero * (1 - q_prime)

  # fit fluctuation/tilting model
  suppressWarnings(
    v_tilt_fit <- stats::glm(
      stats::as.formula("v_pseudo ~ offset(v_star_logit)"),
      data = data.table::as.data.table(list(
        v_pseudo = bound_precision(scale_to_unit(v_pseudo)),
        v_star_logit = v_star_logit
      )),
      subset = data$A == contrast[2],
      weights = data$obs_weights / cv_est$g_star,
      family = "binomial",
      start = 0
    )
  )
  v_star <- stats::plogis(v_star_logit + stats::coef(v_tilt_fit))

  # NOTE: one more round of updating to catch with loop termination
  h_star <- (q_prime / cv_est$r_prime_Z_natural) * h_star_mult
  h_star_Z_one <- (q_prime_Z_one / cv_est$r_prime_Z_one) * h_star_mult
  h_star_Z_zero <- (1 - q_prime_Z_one / (1 - cv_est$r_prime_Z_one)) *
    h_star_mult
  m_prime <- stats::plogis(m_prime_logit + m_coef * h_star)
  m_prime_Z_one <- stats::plogis(m_prime_Z_one_logit + m_coef * h_star_Z_one)
  m_prime_Z_zero <- stats::plogis(m_prime_Z_zero_logit + m_coef *
    h_star_Z_zero)

  # rescale outcome mechanism to original outcome scale
  #m_prime <- scale_to_original(scaled_vals = m_prime,
                               #max_orig = max(data$Y),
                               #min_orig = min(data$Y))
  #m_prime_Z_one <- scale_to_original(scaled_vals = m_prime_Z_one,
                                     #max_orig = max(data$Y),
                                     #min_orig = min(data$Y))
  #m_prime_Z_zero <- scale_to_original(scaled_vals = m_prime_Z_zero,
                                      #max_orig = max(data$Y),
                                      #min_orig = min(data$Y))

  # update pseudo-outcomes and weights for efficient influence function
  u_pseudo <- m_prime * h_star
  v_pseudo <- m_prime_Z_one * q_prime_Z_one + m_prime_Z_zero *
    (1 - q_prime_Z_one)

  # define components of efficient influence function and compute
  eif_y <- (ipw_prime * h_star / mean(ipw_prime * h_star)) * (data$Y - m_prime)
  eif_u <- (ipw_prime / mean(ipw_prime)) * (u_prime_diff) *
    (data$Z - q_prime_Z_one)
  eif_v <- (ipw_star / mean(ipw_star)) * (v_pseudo - v_star)
  eif_est <- (eif_y + eif_u + eif_v + v_star)

  # compute TML estimator and variance from efficient influence function
  if (is.null(ext_weights)) {
    tml_est <- mean(eif_est)
    tmle_var <- stats::var(eif_est) / length(eif_est)
  } else {
    # compute a re-weighted TMLE, with re-weighted influence function
    tml_est <- stats::weighted.mean(eif_est, ext_weights)
    eif_est <- eif_est * ext_weights
    tmle_var <- stats::var(eif_est) / length(eif_est)
  }

  # output
  tml_est_out <- list(
    theta = tml_est,
    var = tmle_var,
    eif = (eif_est - tml_est),
    type = "tmle"
  )
  return(tml_est_out)
}

################################################################################

#' TML EIF for stochastic (in)direct effects under intermediate confounding
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data_ing A \code{data.table} containing the observed data with columns
#'  are in the order specified by the NPSEM (Y, M, Z, A, W), with column names
#'  set appropriately based on the input data. Such a structure is merely a
#'  convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated as part of a call to the user-facing
#'  wrapper function \code{\link{medoutcon}}.
#' @param contrast A \code{numeric} double indicating the two values of the
#'  intervention \code{A} to be compared. The default value of \code{c(0, 1)}
#'  assumes a binary intervention node \code{A}, though support for categorical
#'  interventions is planned for future releases.
#' @param g_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, for use in fitting a model for the propensity
#'  score, i.e., \eqn{g = P(A | W)}.
#' @param e_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a cleverly parameterized
#'  propensity score that includes the mediators, i.e., \eqn{e = P(A | Z, W)}.
#' @param m_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting the outcome regression.
#' @param q_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a regression involving
#'  the mediator-outcome confounder, i.e., \eqn{q(Z | A, W)}.
#' @param r_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a regression involving
#'  the mediator-outcome confounder, i.e., \eqn{r(Z | A, M, W)}.
#' @param u_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a reduced regression
#'  appearing in the efficient influence function, i.e.,
#'  \eqn{u(L, A, W) = E[m(A, Z, M, W) * (q(Z|A,W) / r(Z|A,M,W)) * (e(a'|M,W) /
#'  e(A|M,W)) | Z = z, A = a, W = w]}.
#' @param v_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from the \code{sl3} package, to be used in fitting a reduced regression
#'  useful for computing the efficient one-step estimator, i.e.,
#'  \eqn{v(A,W) = E[\int_z m(a', z, M, W) * q(z | A', W) d\nu(z) | A = a,
#'  W = w)]}.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function
#'  \code{\link{medoutcon}}.
#' @param m_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (M). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{\link{medoutcon}}.
#'
#' @importFrom data.table data.table copy
#' @importFrom origami training validation fold_index
#'
#' @keywords internal
#
cv_eif_tml <- function(fold,
                       data_in,
                       contrast,
                       g_learners,
                       e_learners,
                       m_learners,
                       q_learners,
                       r_learners,
                       u_learners,
                       v_learners,
                       w_names,
                       m_names) {
  # make training and validation data
  train_data <- origami::training(data_in)
  valid_data <- origami::validation(data_in)

  # 1) fit regression for propensity score regression
  g_out <- fit_treat_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = g_learners,
    w_names = w_names,
    m_names = m_names,
    type = "g"
  )

  # 2) fit clever regression for treatment, conditional on mediators
  e_out <- fit_treat_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = e_learners,
    w_names = w_names,
    m_names = m_names,
    type = "e"
  )

  # 3) fit outcome regression
  m_out <- fit_m_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = m_learners,
    m_names = m_names,
    w_names = w_names
  )

  # 4) fit mediator-outcome confounder regression, excluding mediator(s)
  q_out <- fit_moc_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = q_learners,
    m_names = m_names,
    w_names = w_names,
    type = "q"
  )

  # 5) fit mediator-outcome confounder regression, conditioning on mediator(s)
  r_out <- fit_moc_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = r_learners,
    m_names = m_names,
    w_names = w_names,
    type = "r"
  )

  # extract components and re-name for ease of generating influence function
  # NOTE: we only do this for observations in the validation set
  m_prime <- m_out$m_est_valid$m_pred_A_prime
  e_star <- e_out$treat_est_valid$treat_pred_A_star
  g_star <- g_out$treat_est_valid$treat_pred_A_star
  e_prime <- e_out$treat_est_valid$treat_pred_A_prime
  g_prime <- g_out$treat_est_valid$treat_pred_A_prime
  q_prime_Z_one <- q_out$moc_est_valid_Z_one$moc_pred_A_prime
  r_prime_Z_one <- r_out$moc_est_valid_Z_one$moc_pred_A_prime
  q_prime_Z_natural <- q_out$moc_est_valid_Z_natural$moc_pred_A_prime
  r_prime_Z_natural <- r_out$moc_est_valid_Z_natural$moc_pred_A_prime

  # need pseudo-outcome regressions with intervention set to a contrast
  # NOTE: training fits of these nuisance functions must be performed using the
  #       data corresponding to the natural intervention value but predictions
  #       are only needed for u(z,a',w) and v(a*,w) as per the EIF
  valid_data_a_prime <- data.table::copy(valid_data)[, A := contrast[1]]
  valid_data_a_star <- data.table::copy(valid_data)[, A := contrast[2]]
  u_out <- fit_nuisance_u(
    train_data = train_data,
    valid_data = valid_data_a_prime,
    learners = u_learners,
    m_out = m_out,
    g_out = g_out,
    q_out = q_out,
    r_out = r_out,
    e_out = e_out,
    w_names = w_names
  )

  v_out <- fit_nuisance_v(
    train_data = train_data,
    valid_data = valid_data_a_star,
    contrast = contrast,
    learners = v_learners,
    m_out = m_out,
    q_out = q_out,
    m_names = m_names,
    w_names = w_names
  )
  v_pseudo <- v_out$v_pseudo
  v_star <- v_out$v_pred

  # assuming Z in {0,1}, other cases not supported yet
  u_prime_int <- lapply(c(1, 0), function(z_val) {
    # intervene on training and validation data sets
    valid_data_z_interv <- data.table::copy(valid_data)
    valid_data_z_interv[, `:=`(
      Z = z_val,
      A = contrast[1],
      U_pseudo = u_out$u_pred
    )]

    # predict u(z, a', w) using intervened data with treatment set A = a'
    u_task_valid_z_interv <- sl3::sl3_Task$new(
      data = valid_data_z_interv,
      weights = "obs_weights",
      covariates = c("Z", "A", w_names),
      outcome = "U_pseudo",
      outcome_type = "continuous"
    )

    # return partial pseudo-outcome for v nuisance regression
    out_valid <- u_out[["u_fit"]]$predict(u_task_valid_z_interv)
    return(out_valid)
  })
  u_prime_diff <- do.call(`-`, u_prime_int)

  # create inverse probability weights
  ipw_a_prime <- as.numeric(valid_data$A == contrast[1]) / g_prime
  ipw_a_star <- as.numeric(valid_data$A == contrast[2]) / g_star

  # residual term for outcome component of efficient influence function
  h_star <- (g_prime / g_star) * (q_prime_Z_natural / r_prime_Z_natural) *
    (e_star / e_prime)

  # compute un-centered efficient influence function components
  eif_y <- ((ipw_a_prime * h_star) / mean(ipw_a_prime * h_star)) *
    (valid_data$Y - m_prime)
  eif_u <- (ipw_a_prime / mean(ipw_a_prime)) * u_prime_diff *
    (valid_data$Z - q_prime_Z_one)
  eif_v <- (ipw_a_star / mean(ipw_a_star)) * (v_pseudo - v_star)

  # un-centered efficient influence function
  eif <- eif_y + eif_u + eif_v

  # output list
  out <- list(data.table::data.table(
    # components necessary for fluctuation/tilting step of TMLE
    g_prime = g_prime, g_star = g_star, e_prime = e_prime, e_star = e_star,
    q_prime_Z_natural = q_prime_Z_natural, q_prime_Z_one = q_prime_Z_one,
    r_prime_Z_natural = r_prime_Z_natural, r_prime_Z_one = r_prime_Z_one,
    v_star = v_star, u_diff = u_prime_diff, m_prime = m_prime,
    m_prime_Z_zero = v_out$m_A_prime_Z_zero,
    m_prime_Z_one = v_out$m_A_prime_Z_one,
    # efficient influence function and fold-level observation IDs
    D_star = eif, fold = origami::fold_index()
  ))
  return(out)
}
