utils::globalVariables(c("..w_names", "A", "Z"))

#' EIF for interventional (in)direct effects with intermediate confounding
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data_in A \code{data.table} containing the observed data with columns
#'  are in the order specified by the NPSEM (Y, M, Z, A, W), with column names
#'  set appropriately based on the input data. Such a structure is merely a
#'  convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medoutcon}}.
#' @param contrast A \code{numeric} double indicating the two values of the
#'  intervention \code{A} to be compared. The default value of \code{NULL} has
#'  no effect, as the value of the argument \code{effect} is instead used to
#'  define the contrasts. To override \code{effect}, provide a \code{numeric}
#'  double vector, giving the values of a' and a* (e.g., \code{c(0, 1)}.
#' @param g_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a model for the propensity score, i.e.,
#'  \eqn{g = P(A | W)}.
#' @param e_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a cleverly parameterized propensity
#'  score that includes the mediators, i.e., \eqn{e = P(A | Z, W)}.
#' @param m_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting the outcome regression.
#' @param q_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{q(Z | A', W)}.
#' @param r_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{r(Z | A', M, W)}.
#' @param u_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{u(Z, A, W) = E[m(A, Z, M, W) * (q(Z|A,W) / r(Z|A,M,W)) *
#'  (e(a'|M,W) / e(A|M,W)) | Z = z, A = a, W = w]}.
#' @param v_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{v(A,W) = E[\int_m m(a', Z, M, W) * q(z|A',W) d\nu(m) | A=a, W=w)]}.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by \code{\link{medoutcon}}.
#' @param m_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (M). The input for this argument is automatically
#'  generated by \code{\link{medoutcon}}.
#'
#' @importFrom data.table data.table copy
#' @importFrom origami training validation fold_index
#'
#' @keywords internal
cv_eif <- function(fold,
                   data_in,
                   contrast,
                   g_learners,
                   e_learners,
                   m_learners,
                   q_learners,
                   r_learners,
                   u_learners,
                   v_learners,
                   w_names,
                   m_names) {
  # make training and validation data
  train_data <- origami::training(data_in)
  valid_data <- origami::validation(data_in)

  # 1) fit regression for propensity score regression
  g_out <- fit_treat_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = g_learners,
    w_names = w_names,
    m_names = m_names,
    type = "g"
  )

  # 2) fit clever regression for treatment, conditional on mediators
  e_out <- fit_treat_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = e_learners,
    w_names = w_names,
    m_names = m_names,
    type = "e"
  )

  # 3) fit outcome regression
  m_out <- fit_m_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = m_learners,
    m_names = m_names,
    w_names = w_names
  )

  # 4) fit mediator-outcome confounder regression, excluding mediator(s)
  q_out <- fit_moc_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = q_learners,
    m_names = m_names,
    w_names = w_names,
    type = "q"
  )

  # 5) fit mediator-outcome confounder regression, conditioning on mediator(s)
  r_out <- fit_moc_mech(
    train_data = train_data,
    valid_data = valid_data,
    contrast = contrast,
    learners = r_learners,
    m_names = m_names,
    w_names = w_names,
    type = "r"
  )

  # extract components and re-name for ease of generating influence function
  # NOTE: we only do this for observations in the validation set
  m_prime <- m_out$m_est_valid$m_pred_A_prime
  e_star <- e_out$treat_est_valid$treat_pred_A_star
  g_star <- g_out$treat_est_valid$treat_pred_A_star
  e_prime <- e_out$treat_est_valid$treat_pred_A_prime
  g_prime <- g_out$treat_est_valid$treat_pred_A_prime
  q_prime_Z_one <- q_out$moc_est_valid_Z_one$moc_pred_A_prime
  r_prime_Z_one <- r_out$moc_est_valid_Z_one$moc_pred_A_prime
  q_prime_Z_natural <- q_out$moc_est_valid_Z_natural$moc_pred_A_prime
  r_prime_Z_natural <- r_out$moc_est_valid_Z_natural$moc_pred_A_prime

  # need pseudo-outcome regressions with intervention set to a contrast
  # NOTE: training fits of these nuisance functions must be performed using the
  #       data corresponding to the natural intervention value but predictions
  #       are only needed for u(z,a',w) and v(a*,w) as per the EIF
  valid_data_a_prime <- data.table::copy(valid_data)[, A := contrast[1]]
  valid_data_a_star <- data.table::copy(valid_data)[, A := contrast[2]]
  u_out <- fit_nuisance_u(
    train_data = train_data,
    valid_data = valid_data_a_prime,
    learners = u_learners,
    m_out = m_out,
    g_out = g_out,
    q_out = q_out,
    r_out = r_out,
    e_out = e_out,
    w_names = w_names
  )
  u_prime <- u_out$u_pred

  v_out <- fit_nuisance_v(
    train_data = train_data,
    valid_data = valid_data_a_star,
    contrast = contrast,
    learners = v_learners,
    m_out = m_out,
    q_out = q_out,
    m_names = m_names,
    w_names = w_names
  )
  v_star <- v_out$v_pred

  # NOTE: assuming Z in {0,1}, other cases not supported yet
  u_int_eif <- lapply(c(1, 0), function(z_val) {
    # intervene on training and validation data sets
    valid_data_z_interv <- data.table::copy(valid_data)
    valid_data_z_interv[, `:=`(
      Z = z_val,
      A = contrast[1],
      U_pseudo = u_prime
    )]

    # predict u(z, a', w) using intervened data with treatment set A = a'
    u_task_valid_z_interv <- sl3::sl3_Task$new(
      data = valid_data_z_interv,
      weights = "obs_weights",
      covariates = c("Z", "A", w_names),
      outcome = "U_pseudo",
      outcome_type = "continuous"
    )

    # return partial pseudo-outcome for v nuisance regression
    out_valid <- u_out[["u_fit"]]$predict(u_task_valid_z_interv)

    return(out_valid)
  })
  u_int_eif <- do.call(`-`, u_int_eif)

  # create inverse probability weights
  ipw_a_prime <- as.numeric(valid_data$A == contrast[1]) / g_prime
  ipw_a_star <- as.numeric(valid_data$A == contrast[2]) / g_star

  # residual term for outcome component of EIF
  h_star <- (g_prime / g_star) * (q_prime_Z_natural / r_prime_Z_natural) *
    (e_star / e_prime)

  # compute uncentered efficient influence function components
  eif_y <- ipw_a_prime * h_star / mean(ipw_a_prime * h_star) *
    (valid_data$Y - m_prime)
  eif_u <- ipw_a_prime / mean(ipw_a_prime) * u_int_eif *
    (valid_data$Z - q_prime_Z_one)
  eif_v <- ipw_a_star / mean(ipw_a_star) * (v_out$v_pseudo - v_star)

  # un-centered efficient influence function
  eif <- eif_y + eif_u + eif_v + v_star

  # output list
  out <- list(data.table::data.table(
    # components necessary for fluctuation/tilting step of TMLE
    g_prime = g_prime, g_star = g_star, e_prime = e_prime, e_star = e_star,
    q_prime_Z_natural = q_prime_Z_natural, q_prime_Z_one = q_prime_Z_one,
    r_prime_Z_natural = r_prime_Z_natural, r_prime_Z_one = r_prime_Z_one,
    v_star = v_star, u_int_diff = u_int_eif, m_prime = m_prime,
    m_prime_Z_zero = v_out$m_A_prime_Z_zero,
    m_prime_Z_one = v_out$m_A_prime_Z_one,
    # efficient influence function and fold IDs
    D_star = eif, fold = origami::fold_index()
  ))
  return(out)
}

###############################################################################

#' One-step for interventional (in)direct effects with intermediate confounding
#'
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, M, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medoutcon}}.
#' @param contrast A \code{numeric} double indicating the two values of the
#'  intervention \code{A} to be compared. The default value of \code{NULL} has
#'  no effect, as the value of the argument \code{effect} is instead used to
#'  define the contrasts. To override \code{effect}, provide a \code{numeric}
#'  double vector, giving the values of a' and a* (e.g., \code{c(0, 1)}.
#' @param g_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a model for the propensity score, i.e.,
#'  \eqn{g = P(A | W)}.
#' @param e_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a cleverly parameterized propensity
#'  score that includes the mediators, i.e., \eqn{e = P(A | Z, W)}.
#' @param m_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting the outcome regression.
#' @param q_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{q(Z | A', W)}.
#' @param r_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{r(Z | A', M, W)}.
#' @param u_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{u(Z, A, W) = E[m(A, Z, M, W) * (q(Z|A,W) / r(Z|A,M,W)) *
#'  (e(a'|M,W) / e(A|M,W)) | Z = z, A = a, W = w]}.
#' @param v_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{v(A,W) = E[\int_m m(a', Z, M, W) * q(z|A',W) d\nu(m) | A=a, W=w)]}.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by \code{\link{medoutcon}}.
#' @param m_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (M). The input for this argument is automatically
#'  generated by \code{\link{medoutcon}}.
#' @param ext_weights A \code{numeric} vector of observation-level weights that
#'  have been computed externally. Such weights are used in the construction of
#'  a re-weighted estimator.
#' @param cv_folds A \code{numeric} integer specifying the number of folds to
#'  be created for cross-validation. Use of cross-validation allows for entropy
#'  conditions on the one-step estimator to be relaxed. For compatibility with
#'  \code{\link[origami]{make_folds}}, this value specified must be greater
#'  than or equal to 2; the default is to create 10 folds.
#'
#' @importFrom assertthat assert_that
#' @importFrom stats var weighted.mean
#' @importFrom origami make_folds cross_validate folds_vfold
#'
#' @export
est_onestep <- function(data,
                        contrast,
                        g_learners,
                        e_learners,
                        m_learners,
                        q_learners,
                        r_learners,
                        u_learners,
                        v_learners,
                        w_names,
                        m_names,
                        ext_weights = NULL,
                        cv_folds = 10) {

  # make sure that more than one fold is specified
  assertthat::assert_that(cv_folds > 1)

  # create folds for use with origami::cross_validate
  folds <- origami::make_folds(data,
    fold_fun = origami::folds_vfold,
    V = cv_folds
  )

  # perform the cv_eif procedure on a per-fold basis
  cv_eif_results <- origami::cross_validate(
    cv_fun = cv_eif,
    folds = folds,
    data_in = data,
    contrast = contrast,
    g_learners = g_learners,
    e_learners = e_learners,
    m_learners = m_learners,
    q_learners = q_learners,
    r_learners = r_learners,
    u_learners = u_learners,
    v_learners = v_learners,
    w_names = w_names,
    m_names = m_names,
    use_future = FALSE,
    .combine = FALSE
  )

  # get estimated observation-level values of efficient influence function
  eif_est <- do.call(c, lapply(cv_eif_results[[1]], `[[`, "D_star"))

  # compute one-step estimate and variance from efficient influence function
  if (is.null(ext_weights)) {
    os_est <- mean(eif_est)
    os_var <- stats::var(eif_est) / length(eif_est)
  } else {
    # compute a re-weighted one-step, with re-weighted influence function
    os_est <- stats::weighted.mean(eif_est, ext_weights)
    eif_est <- eif_est * ext_weights
    os_var <- stats::var(eif_est) / length(eif_est)
  }

  # output
  os_est_out <- list(
    theta = os_est,
    var = os_var,
    eif = (eif_est - os_est),
    type = "onestep"
  )
  return(os_est_out)
}

###############################################################################

#' TMLE for interventional (in)direct effects under intermediate confounding
#'
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, M, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medoutcon}}.
#' @param contrast A \code{numeric} double indicating the two values of the
#'  intervention \code{A} to be compared. The default value of \code{NULL} has
#'  no effect, as the value of the argument \code{effect} is instead used to
#'  define the contrasts. To override \code{effect}, provide a \code{numeric}
#'  double vector, giving the values of a' and a* (e.g., \code{c(0, 1)}.
#' @param g_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a model for the propensity score, i.e.,
#'  \eqn{g = P(A | W)}.
#' @param e_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a cleverly parameterized propensity
#'  score that includes the mediators, i.e., \eqn{e = P(A | Z, W)}.
#' @param m_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting the outcome regression.
#' @param q_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, for use in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{q(Z | A', W)}.
#' @param r_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a regression involving the
#'  mediator-outcome confounder, i.e., \eqn{r(Z | A', M, W)}.
#' @param u_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{u(Z, A, W) = E[m(A, Z, M, W) * (q(Z|A,W) / r(Z|A,M,W)) *
#'  (e(a'|M,W) / e(A|M,W)) | Z = z, A = a, W = w]}.
#' @param v_learners A \code{Stack} object, or other learner class (inheriting
#'  from \code{Lrnr_base}), containing a single or set of instantiated learners
#'  from \pkg{sl3}, to be used in fitting a reduced regression, i.e.,
#'  \eqn{v(A,W) = E[\int_m m(a', Z, M, W) * q(z|A',W) d\nu(m) | A=a, W=w)]}.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by \code{\link{medoutcon}}.
#' @param m_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (M). The input for this argument is automatically
#'  generated by \code{\link{medoutcon}}.
#' @param ext_weights A \code{numeric} vector of observation-level weights that
#'  have been computed externally. Such weights are used in the construction of
#'  a re-weighted estimator.
#' @param cv_folds A \code{numeric} value specifying the number of folds to be
#'  created for cross-validation. Use of cross-validation allows for entropy
#'  conditions on the TML estimator to be relaxed. Note: for compatibility with
#'  \code{\link[origami]{make_folds}}, this value  must be greater than or
#'  equal to 2; the default is to create 10 folds.
#' @param max_iter A \code{numeric} integer giving the maximum number of steps
#'  to be taken for the iterative procedure to construct a TML estimator.
#'
#' @importFrom dplyr "%>%"
#' @importFrom assertthat assert_that
#' @importFrom stats var glm as.formula qlogis plogis coef weighted.mean
#' @importFrom origami make_folds cross_validate folds_vfold
#'
#' @export
est_tml <- function(data,
                    contrast,
                    g_learners,
                    e_learners,
                    m_learners,
                    q_learners,
                    r_learners,
                    u_learners,
                    v_learners,
                    w_names,
                    m_names,
                    ext_weights = NULL,
                    cv_folds = 10,
                    max_iter = 5,
                    tiltmod_tol = 100) {

  # make sure that more than one fold is specified
  assertthat::assert_that(cv_folds > 1)

  # create folds for use with origami::cross_validate
  folds <- origami::make_folds(data,
    fold_fun = origami::folds_vfold,
    V = cv_folds
  )

  # perform the cv_eif procedure on a per-fold basis
  cv_eif_results <- origami::cross_validate(
    cv_fun = cv_eif,
    folds = folds,
    data_in = data,
    contrast = contrast,
    g_learners = g_learners,
    e_learners = e_learners,
    m_learners = m_learners,
    q_learners = q_learners,
    r_learners = r_learners,
    u_learners = u_learners,
    v_learners = v_learners,
    w_names = w_names,
    m_names = m_names,
    use_future = FALSE,
    .combine = FALSE
  )

  # concatenate nuisance function and influence function estimates across folds
  cv_eif_est <- do.call(rbind, cv_eif_results[[1]])
  obs_valid_idx <- do.call(c, lapply(folds, `[[`, "validation_set"))
  cv_eif_est <- cv_eif_est[obs_valid_idx, ]

  # extract nuisance function estimates and auxiliary quantities
  g_prime <- cv_eif_est$g_prime
  e_prime <- cv_eif_est$e_prime
  g_star <- cv_eif_est$g_star
  e_star <- cv_eif_est$e_star
  q_prime_Z_one <- cv_eif_est$q_prime_Z_one
  q_prime_Z_natural <- cv_eif_est$q_prime_Z_natural
  r_prime_Z_one <- cv_eif_est$r_prime_Z_one
  r_prime_Z_natural <- cv_eif_est$r_prime_Z_natural
  m_prime_Z_one <- cv_eif_est$m_prime_Z_one
  m_prime_Z_zero <- cv_eif_est$m_prime_Z_zero
  m_prime_Z_natural <- cv_eif_est$m_prime

  # generate inverse weights
  ipw_prime <- as.numeric(data$A == contrast[1]) / g_prime
  ipw_star <- as.numeric(data$A == contrast[2]) / g_star

  # prepare for iterative targeting
  n_iter <- 0
  eif_stop_crit <- FALSE
  tilt_stop_crit <- 0.001 / log(nrow(data))

  # perform iterative targeting for TMLE
  while (!eif_stop_crit && n_iter <= max_iter) {
    # compute auxiliary covariate H*
    h_star_mult <- (g_prime / g_star) * (e_star / e_prime)
    h_star_Z_one <- (q_prime_Z_one / r_prime_Z_one) * h_star_mult
    h_star_Z_zero <- ((1 - q_prime_Z_one) / (1 - r_prime_Z_one)) * h_star_mult
    h_star_Z_natural <- (q_prime_Z_natural / r_prime_Z_natural) * h_star_mult

    # re-scale and transform nuisance estimates for tilting models
    m_prime_Z_natural_logit <- m_prime_Z_natural %>%
      scale_to_unit() %>%
      bound_precision() %>%
      stats::qlogis()
    m_prime_Z_one_logit <- m_prime_Z_one %>%
      scale_to_unit() %>%
      bound_precision() %>%
      stats::qlogis()
    m_prime_Z_zero_logit <- m_prime_Z_zero %>%
      scale_to_unit() %>%
      bound_precision() %>%
      stats::qlogis()
    q_prime_Z_one_logit <- q_prime_Z_one %>%
      scale_to_unit() %>%
      bound_precision() %>%
      stats::qlogis()

    # fit first tilting model - for the outcome mechanism
    suppressWarnings(
      m_tilt_fit <- stats::glm(
        stats::as.formula("y_scaled ~ -1 + offset(m_prime_logit) + h_star"),
        data = data.table::as.data.table(list(
          A = data$A,
          y_scaled = data$Y,
          m_prime_logit = m_prime_Z_natural_logit,
          h_star = h_star_Z_natural
        )),
        subset = data$A == contrast[1],
        weights = data$obs_weights / g_prime,
        family = "binomial",
        start = 0
      )
    )
    if (!m_tilt_fit$converged | abs(max(stats::coef(m_tilt_fit))) >
        tiltmod_tol) {
      m_tilt_fit$coefficients <- 0
    }
    m_tilt_coef <- unname(stats::coef(m_tilt_fit))

    # fit second tilting model - for the intermediate confounding mechanism
    suppressWarnings(
      q_tilt_fit <- stats::glm(
        stats::as.formula("Z ~ -1 + offset(q_prime_logit) + u_prime_diff"),
        data = data.table::as.data.table(list(
          A = data$A,
          Z = data$Z,
          u_prime_diff = cv_eif_est$u_int_diff,
          q_prime_logit = q_prime_Z_one_logit
        )),
        subset = data$A == contrast[1],
        weights = data$obs_weights / g_prime,
        family = "binomial",
        start = 0
      )
    )
    if (!q_tilt_fit$converged | abs(max(stats::coef(q_tilt_fit))) >
        tiltmod_tol) {
      q_tilt_fit$coefficients <- 0
    }
    q_tilt_coef <- unname(stats::coef(q_tilt_fit))

    # update nuisance estimates via tilting models, for outcome
    m_prime_Z_natural <- stats::plogis(m_prime_Z_natural_logit +
                                       m_tilt_coef * h_star_Z_natural)
    #%>%
      #scale_from_unit(max_orig = max(m_prime_Z_natural),
                      #min_orig = min(m_prime_Z_natural))
    m_prime_Z_one <- stats::plogis(m_prime_Z_one_logit +
                                   m_tilt_coef * h_star_Z_one)
    #%>%
      #scale_from_unit(max_orig = max(m_prime_Z_one),
                      #min_orig = min(m_prime_Z_one))
    m_prime_Z_zero <- stats::plogis(m_prime_Z_zero_logit +
                                    m_tilt_coef * h_star_Z_zero)
    #%>%
      #scale_from_unit(max_orig = max(m_prime_Z_zero),
                      #min_orig = min(m_prime_Z_zero))

    # update nuisance estimates via tilting models, for intermediate confounder
    q_prime_Z_one <- stats::plogis(q_prime_Z_one_logit + q_tilt_coef *
                                   cv_eif_est$u_int_diff)
    #%>%
      #scale_from_unit(max_orig = max(q_prime_Z_one),
                      #min_orig = min(q_prime_Z_one))
    q_prime_Z_natural <- data$Z * q_prime_Z_one + (1 - data$Z) *
      (1 - q_prime_Z_one)

    # iterate the iterator and check convergence
    n_iter <- n_iter + 1
    eif_stop_crit <- max(abs(c(m_tilt_coef, q_tilt_coef))) < tilt_stop_crit
  }

  # one more update to catch loop termination
  h_star_Z_natural <- (q_prime_Z_natural / r_prime_Z_natural) * h_star_mult

  # compute updated substitution estimator and prepare for tilting regression
  v_star_logit <- cv_eif_est$v_star %>%
    scale_to_unit() %>%
    bound_precision() %>%
    stats::qlogis()
  v_pseudo <- ((m_prime_Z_one * q_prime_Z_one) +
    (m_prime_Z_zero * (1 - q_prime_Z_one))) %>%
    scale_to_unit() %>%
    bound_precision()

  # fit tilting model for substitution estimator
  suppressWarnings(
    v_tilt_fit <- stats::glm(
      stats::as.formula("v_pseudo ~ offset(v_star_logit)"),
      data = data.table::as.data.table(list(
        A = data$A,
        v_pseudo = v_pseudo,
        v_star_logit = v_star_logit
      )),
      subset = data$A == contrast[2],
      weights = data$obs_weights / g_star,
      family = "binomial",
      start = 0
    )
  )
  #browser()
  v_star <- stats::plogis(v_star_logit + stats::coef(v_tilt_fit))
  #%>%
      #scale_from_unit(max_orig = max(cv_eif_est$v_star),
                      #min_orig = min(cv_eif_est$v_star))

  # define updated components of efficient influence function
  eif_y <- (data$Y - m_prime_Z_natural) * (ipw_prime * h_star_Z_natural /
                                           mean(ipw_prime * h_star_Z_natural))
  eif_u <- (data$Z - q_prime_Z_one) * (ipw_prime / mean(ipw_prime)) *
    cv_eif_est$u_int_diff
  eif_v <- (v_pseudo - v_star) * (ipw_star / mean(ipw_star))
  eif_est <- (eif_y + eif_u + eif_v + v_star)

  # compute TML estimator and variance from efficient influence function
  if (is.null(ext_weights)) {
    tml_est <- mean(eif_est)
    tmle_var <- stats::var(eif_est) / length(eif_est)
  } else {
    # compute a re-weighted TMLE, with re-weighted influence function
    tml_est <- stats::weighted.mean(eif_est, ext_weights)
    eif_est <- eif_est * ext_weights
    tmle_var <- stats::var(eif_est) / length(eif_est)
  }

  # output
  tmle_out <- list(
    theta = tml_est,
    var = tmle_var,
    eif = (eif_est - tml_est),
    type = "tmle"
  )
  return(tmle_out)
}
